{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-096FPSGRZzA"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # Vehicle Telemetry Analytics - Advanced Analysis & Insights\n",
        "#\n",
        "# ## Executive Summary\n",
        "# This notebook performs comprehensive analysis on engineered features to extract actionable insights, build predictive models, and generate business recommendations for vehicle fleet optimization.\n",
        "#\n",
        "# ## Key Objectives\n",
        "# 1. Predictive Modeling for Maintenance\n",
        "# 2. Fuel Efficiency Analysis\n",
        "# 3. Driver Behavior Scoring\n",
        "# 4. Anomaly Detection at Scale\n",
        "# 5. Business Insights Generation\n",
        "# 6. Dashboard Preparation\n",
        "#\n",
        "# ## Technologies Used\n",
        "# - XGBoost, LightGBM, CatBoost for predictive modeling\n",
        "# - SHAP, LIME for model interpretation\n",
        "# - Prophet for time series forecasting\n",
        "# - MLflow for experiment tracking\n",
        "# - Streamlit/Tableau dashboard preparation\n",
        "\n",
        "# %% [code]\n",
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn xgboost lightgbm catboost shap lime optuna mlflow prophet -q\n",
        "!pip install plotly dash streamlit pycaret -q\n",
        "\n",
        "# %% [code]\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, confusion_matrix, classification_report,\n",
        "                           mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
        "                            IsolationForest, VotingClassifier, StackingClassifier)\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "\n",
        "# Advanced Models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "from prophet import Prophet\n",
        "\n",
        "# Model Interpretation\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "\n",
        "# Time Series\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Experiment Tracking\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "\n",
        "# %% [code]\n",
        "# Load engineered features\n",
        "try:\n",
        "    telemetry_df = pd.read_csv('engineered_features/telemetry_engineered.csv')\n",
        "    print(f\"‚úÖ Loaded engineered dataset: {telemetry_df.shape}\")\n",
        "\n",
        "    # Load feature importance\n",
        "    import json\n",
        "    with open('engineered_features/feature_importance.json', 'r') as f:\n",
        "        feature_importance = json.load(f)\n",
        "\n",
        "    print(\"‚úÖ Loaded feature importance metadata\")\n",
        "\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è No engineered features found. Creating sample dataset...\")\n",
        "\n",
        "    # Create comprehensive sample dataset\n",
        "    np.random.seed(42)\n",
        "    n_samples = 10000\n",
        "\n",
        "    telemetry_df = pd.DataFrame({\n",
        "        'vehicle_id': np.random.choice([f'VH{str(i).zfill(3)}' for i in range(1, 21)], n_samples),\n",
        "        'timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='1min'),\n",
        "        'speed_kmh': np.random.gamma(shape=2, scale=15, size=n_samples) + 20,\n",
        "        'engine_rpm': np.random.normal(2500, 500, n_samples),\n",
        "        'fuel_consumption_lph': np.random.exponential(5, n_samples) + 3,\n",
        "        'engine_temp_c': np.random.normal(90, 5, n_samples),\n",
        "        'oil_temp_c': np.random.normal(85, 3, n_samples),\n",
        "        'coolant_temp_c': np.random.normal(88, 4, n_samples),\n",
        "        'battery_voltage': np.random.normal(12.5, 0.5, n_samples),\n",
        "        'throttle_position': np.random.uniform(0, 100, n_samples),\n",
        "        'brake_pressure': np.random.exponential(10, n_samples),\n",
        "        'vehicle_load_kg': np.random.choice([1000, 1500, 2000, 2500], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
        "        'fuel_level': np.random.uniform(10, 100, n_samples),\n",
        "        'hour': np.random.randint(0, 24, n_samples),\n",
        "        'day_of_week': np.random.randint(0, 7, n_samples),\n",
        "        'is_weekend': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),\n",
        "        'is_business_hours': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
        "        'engine_stress_score': np.random.uniform(0, 1, n_samples),\n",
        "        'battery_health_score': np.random.uniform(0.5, 1, n_samples),\n",
        "        'instant_fuel_efficiency': np.random.uniform(5, 25, n_samples),\n",
        "        'overheating_risk': np.random.choice([0, 1], n_samples, p=[0.95, 0.05]),\n",
        "        'low_battery_warning': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
        "        'speeding_indicator': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
        "        'vehicle_health_score': np.random.uniform(0.6, 1, n_samples),\n",
        "        'maintenance_required': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
        "        'target_failure': np.random.choice([0, 1], n_samples, p=[0.9, 0.1])\n",
        "    })\n",
        "\n",
        "    print(f\"üìä Created sample dataset: {telemetry_df.shape}\")\n",
        "\n",
        "print(f\"\\nüìã Dataset Columns: {list(telemetry_df.columns)}\")\n",
        "\n",
        "# %% [code]\n",
        "# Advanced Predictive Modeling for Maintenance\n",
        "class MaintenancePredictor:\n",
        "    \"\"\"\n",
        "    Advanced predictive maintenance modeling system\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.feature_importance = {}\n",
        "\n",
        "    def prepare_data(self, df, target_col='maintenance_required', test_size=0.2):\n",
        "        \"\"\"\n",
        "        Prepare data for modeling\n",
        "        \"\"\"\n",
        "        print(\"üìä Preparing data for modeling...\")\n",
        "\n",
        "        # Separate features and target\n",
        "        if target_col not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è Target column '{target_col}' not found. Using 'target_failure'\")\n",
        "            target_col = 'target_failure'\n",
        "\n",
        "        X = df.drop(columns=[target_col, 'vehicle_id', 'timestamp']\n",
        "                   if 'vehicle_id' in df.columns and 'timestamp' in df.columns\n",
        "                   else [target_col])\n",
        "        y = df[target_col]\n",
        "\n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.mean())\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        print(f\"  Training set: {X_train.shape}\")\n",
        "        print(f\"  Test set: {X_test.shape}\")\n",
        "        print(f\"  Class distribution - Train: {y_train.value_counts().to_dict()}\")\n",
        "        print(f\"  Class distribution - Test: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, X.columns.tolist()\n",
        "\n",
        "    def train_models(self, X_train, X_test, y_train, y_test, feature_names):\n",
        "        \"\"\"\n",
        "        Train multiple advanced models\n",
        "        \"\"\"\n",
        "        print(\"\\nü§ñ Training advanced models...\")\n",
        "\n",
        "        models_to_train = {\n",
        "            'Random Forest': RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=10,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                class_weight='balanced',\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            ),\n",
        "            'XGBoost': xgb.XGBClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=42,\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='logloss'\n",
        "            ),\n",
        "            'LightGBM': lgb.LGBMClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                num_leaves=31,\n",
        "                random_state=42,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'CatBoost': cb.CatBoostClassifier(\n",
        "                iterations=200,\n",
        "                depth=6,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42,\n",
        "                verbose=0\n",
        "            ),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=5,\n",
        ""
      ]
    }
  ]
}